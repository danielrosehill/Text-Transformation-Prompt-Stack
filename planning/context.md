# Transcript: testing_compressed.mp3

So the purpose of this voice note is going to be to provide some context in audio format as it's an audio workflow, describing basically I'm going to use this audio as actually kind of an internal test in the repository, to test out the various prompts that I write for this job. And it's a prompt workflow that I would say is kind of deceptively it's more challenging than it looks. And what I'm trying to do in this repository, there's a couple of purposes. The it's to support a number of different voice projects I'm working on and a common requirement or that I've kind of seen over the course of the last year of using this approach in practice is nailing, as it were, the prompt for text rewriting with an audio multimodal model like Gemini. And the apps that I'm working on, the approaches I'm working on are basically using these for voice transcription. So using the audio understanding capability of audio multimodal and really using it as a drop-in replacement for Whisper, and not just for Whisper, for Whisper and then LLM. So that voice pipeline that is what I've been iterating on. Previously it involves, you send your audio to Whisper, you get back your verbatim transcript, and then you say, hang on, I want to polish this a bit.

And there's two different types of polish as I see it. The first one is what I would regard as generally desirable polish, to stick with that analogy just for a second, that word, which is that if I transcribe something in Whisper, there's, there are a couple of edge cases where you want a verbatim reproduction of text. Just as an example, in court proceedings let's say. So, I'm sure there's a lot of other ones where you need for legal reasons to have an exact copy of what was said. And I'm sure in those use cases it's probably human based. I'm sure AI is leveraged, but that's not the one that I'm using or that I'm thinking about. The one I'm thinking about is I'm at my computer and I'm trying to, I'm basically in the process of you know, replacing typing with voice. And that means that I want to record voice recordings for that are intended to be sent as emails or written out as blogs or read me's, or increasingly prompts has been probably my major use case, development prompts, in which it would take me a long time to write out all the instructions, and it's just easier if I'm describing let's say CSS edits or UI UX edits. It's much easier for me to just start recording, go through the UI, and say this needs fixing, this needs fixing, than it would to try to replicate that process textually, like going through pages and writing notes. So, the use cases kind of span the gamut really, but the idea in defining this prompt stack in its own repository was that as I kind of iterated upon this concept and worked through it, I've come to identify different things over time that I think should be integrated.

And it really does become a stack, that it's a stack of instructions that the more complex it gets, the harder it gets to remember all the different things you wanted and the less practical it becomes really to do that manually. So the idea of this is programmatically, is to define the levels in an orderly fashion. And I'm actually using AI for its reasoning to say, okay, the order is is is important. The content is important, but so is the order. Like what is the, what is the appropriate sequence to send these instructions to the model, so that it makes sense. So for example, one of the layers in the prompt stack is personalization, which is let's say I'm using this for writing emails. In the voice notepad tool that I'm currently working with, I've added a setting for the username and the user signature. And the reason I added that setting was that when I'm writing emails, basically, you know, the meta objective here is that you want to get transcriptions that are ideally ready to go without any edits needed. And that's quite a bit beyond what you might get with Whisper, whereby it's raw and there's various levels needed for you to actually send it. One pitfall I've seen in in the email apps that I use, the voice notepad apps that I use myself, is that lack of personalization where you don't doesn't have the user's name, the AI model in that case will have no choice but to write "Regards, your name". And besides that being something you need to correct, it creates the danger whereby you'll not notice that, send the email, and it will be embarrassing and unprofessional. So the objective of the personal information setting in the, in the utility here that I'm writing this in with that utility in mind for, is that we'll give the model, you know, we're if we're using a model like Gemini with a lot of reasoning.

I'm trying to write a stack here, a prompt, a metaprompt that really takes advantage of that. So, you know, it's saying, Gemini, if you if you see that this is an email, you have the user's name, put it in. So say "Regards, Daniel". Don't say "Regards, your name". And in the prompt stack, that's reflected in a series of the, some of the instructions are inferred, which means that it's asking the model to actually use its reasoning to infer an instruction. An example is, if I say in the course, and I've added upon this list of examples today in V2, an example in V1 of an inferred instruction was, if I say, I'm going to the supermarket today. Wait, no. I meant to say I'm going to the pharmacy. And the inference requested is that the model says, okay, we don't need to capture all of that. We just need to record in our text "I'm going to the pharmacy". The one that I came up with today was sometimes I spell things, and this is again, just little little things I've over the course of the year come to best practices, or, you know, tricks I've developed, is sometimes if I say something that I just think the AI tool is probably going to transcribe incorrectly because it's not a common word, I'll spell it out. So I'll say today I was talking about Zod, which is a TypeScript error validator, linter. And, you know, I'll say, let's use Zod here. Zod is spelled Z O D.

Now the desired output of that text is just, let's use Zod here. We don't need to capture in the text output the spelling. We just need to infer that as an instruction. And so those are two inferred instructions that are added into the stack. There was also one today, I made it, I made a grammatical error, like I said, we need to add this to the list of option. That's actually what I said. I said option instead of options for some reason. And there's kind of a layer in the stack that says correct basic text typos. But that you have to think that that's, some of these are kind of with audio understanding, it's very much changed the dynamic of the workflow. Because I'm probably thinking when I'm writing these prompts of I'm still thinking of the Whisper LLM workflow, whereby it's getting something from Whisper and the model has to say, hang on, that was a mistranscription. In the case of audio multimodal, that's not applicable because the model is simultaneously processing the audio tokens and the text instructions. So it's the only cook in the kitchen, which I think actually makes an awful lot of sense and not only does it streamline the workflow, and that's why I'm rearchitecting from Whisper over to this, and just by way of as an aside, I added to the tool, the tool that I'm using, voice notepad on, on, on GitHub, is actually kind of a prototype in action because I've been using it now for a couple of weeks and I've got over 800 transcriptions, which means I'm using it just for everything, little bursts of things, mostly prompts I would say, 80%. And I've spent I think like a dollar or $2 in OpenRouter API, which I'm using simply because it allows you to track your expenditure at the key level, which Gemini doesn't. So I can, I can issue an API key just for this app, go into the cost tracking and see precisely how much I've spent across all models. And that's not even using the cheapest Gemini model. I'm using Gemini flash 2.5 regular. There's also Pro, there's also Light. There's also Gemini 3. To be honest, I don't see any, there's no reason at that level of cost for me personally to try to do it any cheaper. That's more than acceptable. There's also no compelling reason to use Pro because it's doing a good enough job, and it's probably, it would probably be a total waste. In fact, I would say that the, the, the model that Google would probably recommend itself for this workflow is Light, because it's a really, you just need something that can process audio tokens, process your instruction. The only cost optimization that I can think of that would actually make sense here is prompt caching, because of the fact that the general cleaner prompt in the way that I'm envisioning this is quite extensive but also repetitive. So if I'm recording a short transcript like, let's take this UI and let's edit the CSS and add these change these colors to this. That would be a very short prompt and it's kind of stupid to send 5 seconds of audio alongside a 600 word prompt defining all the, you know, if it says this, change this, it's overkill. So that's really I guess an engineering question. Maybe it's a front end thing that I want to have like if it's very short, the audio will send a short cleaner prompt. And if it's more than 1 minute, we'll send like kind of the bigger one. But the only real, real reason for that would just be kind of cost optimization because it doesn't make sense to me. But, you know, when you're talking, the amount of, the costs are so little that it's almost just makes more sense to process the text. So this is the sample audio that I've created as an example of a long note.

And what I'm recording this for beyond explaining the idea here, oh, I should have added as well actually. So that's the number one idea, the most important one I would say is versioning the, the general cleanup prompt. And the reason that I'm doing this in a version controlled way is that as I mentioned, is mostly static, but I do make tweaks over time like the one today where I said, okay, when I'm getting back in the course of using the system myself, I got back a transcript saying that should be spelled Z O D and I'm like, wait, I should add that as an instruction because I don't need to see that. I just want to have that applied. So a prompt, there's going to be a constructor here that the most important thing that I want to do really I think is define the sequence of the constructed or consolidated or concatenated prompt. That makes the most sense because as I said, I think that's important not just for presentation, but I think I want to give the LLM and I want to add subheadings. That's another edition I'm making today, so that it kind of just based on what I've noticed with the working with LLM's that they, they love structure and the more you can help them with headings and subheadings, the more the better you'll get. That's my theory and I think there's some evidence for it. You'll get better adherence there. And finally then for the, the second thing is is generating out the constructed prompts with, with variants. So what I've tried to do is divide it at the top level between foundational and specific prompts, specific ones being there's really actually kind of a world of permutations that you can do here, in the realm of specific prompts. And I'm sure I've only thought of a few of them. The ones that I, that I use in this application that I, that I would kind of, I'm trying to like present as these are the few ones you'll want to use 90% of the time: to-do list, system prompt, user prompt, development prompts. And something else that I would say by way of an aside again, and maybe with if we tune the prompt a bit for thought organization, we can make my own notes here a bit more organized. By way of another aside, what I've noticed over the course of time is that if you can really get the general system prompt very effective, a lot of these distinctions don't really matter. Like you know, an AI prompt in a sense is specific, yes, but to a greater extent it also adheres to principles of good writing.

And so if you can have a good foundational prompt that clears up the, you know, the filler words, add subheadings, resolves typos, the rest of it is probably going to be suitable as an AI prompt, without you needing to say add in like optimize this for its intended use as an AI prompt. For email, it's a little bit more, that's one where I do see it, a definite need for the format specific instruction because an email needs to be written in a format like "Dear X", you want to have short paragraphs. And that's as well for the stylistic instructions where I have presets for business, business appropriate, casual appropriate. And that, those are more the cases where if you don't have that added tweak in the format specific versions, it can go off because you know, you don't want to write something like really casual to as a work email. I'm going to have to pick up our little Ezra here. So this is a good example. And this is another, this is a perfect, perfect thing to happen because this actually takes us to inferred instructions. I just mentioned I'm picking up my son Ezra, who I'm now carrying and I'm holding up my phone to dictate. And probably, I don't need that in the transcripts. I don't even know why I said it. I'm just in the habit of it, I guess. And I feel like I'm recording this in case someone might want to listen to it. But depending on the context, that could probably be safely scrubbed. Another one in the inferred instruction category is when you're transcribing, when you're dictating something, you might, I don't know, you're in an office context. A friend bumps in and says, oh hey Dan, and then you gesture to your microphone and you're like, hey, I'm in the middle of something, can we do this later? So again, an example of something that with audio understanding, you're actually, it can hear that was a different person, and I'm asking it to reason to say, we don't need that in the transcript. If the user was recording a blog instruction or a document, we don't need to to get that part where the friend popped into the office and the user responded or or the DHL guy, et cetera. So this category of inferred instructions for removal, I would say this is actually deductive logic. This is asking the AI tool, the reasoning model to deduce that we don't need this.

Speaking of pure inferred logic, another one that I thought of today, and this is again where this workflow gets risky because the more you're asking an AI model to use its own reasoning, potentially the more useful it is, but the potential for it to destroy the transcript gets higher. So it's a gamble basically. But the one I thought of today was as maybe an alternative to the stylistic instruction prompt concatenation, whereby the user selects a button saying this is an email. The AI model is asked to deduce that itself. The user might say, this is an email to my boss. The AI tool can say, okay, this is a context. That's a formatting directive and I'm going to provide the transcript with that in mind. I'm going to therefore make sure it's business appropriate. I'm going to include the user's email to signature. That's much more elegant in fact. And I think probably that's the end goal for this pattern and implementation because, you know, that would eliminate the need for buttons entirely if the user could just speak and define the needs and we don't need to have a button for the user to have to use to define the purpose. The other one would be at the more ambitious level of scale, sometimes I've thought about capturing stuff in JSON. Like I might define something I see or I'm, I don't know, recording some information. Trying to think. I mean, it's kind of a very marginal, marginal idea. But, you could say I'm creating a home inventory system and I want to record what's in my house and looking around, here's what I see. And then the, the constructed prompt might be create a JSON array for the categories of goods in the user's house. And that might be used to jumpstart the development of, you know, home inventory system. But again, this is, these are edge cases but ones that are particularly interesting because they open up the possibility that the transcripts can not only be human readable but also machine readable for use in AI applications. But the main workflow I'm working on is actually doing that as a second layer, as a processing layer in order that we can connect these voice transcripts to AI tools. This is more pure, this is more utility for humans to use.

I think that pretty much covers the concept behind the prompt stack, as I call it. And the reason that there's a divergence between the, what I call the foundational one. And the other thing, the other kind of let's say I don't know what the right word is. Metatype, if you will, is actually a verbatim transcript. And with a model like Whisper, an ASR model, you don't need to do anything to get that. That's what you're going to get by default. You're going to get a verbatim transcript. With an audio multimodal model with audio understanding, unless the API is, you know, designed to accommodate this, you need to tell the model what to do with that audio. If I just send it audio without any prompt, it doesn't know. Well, what do you want? You want me to identify the speaker? So to get a verbatim transcript, with audio multimodal, you actually need to explicitly create a verbatim transcript prompt. And therefore that's one that I wanted to, that I have in the app that I'm working on. And there are again use cases for that. I would say in general, it's less makes it much less sense with audio multimodal for the simple reason that if that was your objective, you'd probably just be using a traditional ASR approach. But if you were happy to kind of gamble on audio multimodal for all of this, you might have that as a prompt. Now it's going to be again less, because you're prompting for that. I think it's going to be less 100% robust than than Whisper would be. But there might be some uses and therefore that's one that I have in the app as well that I want to include in the pipeline. So but the main, the main one I would say, 90% what I've discovered in this is that you know, it's all these presets that I've thought of and I didn't even get into them. There's there's a bunch of them. I mean, there is you could play around with word constraints. So and the, the general flow in the prompt stack is that these would come after the foundational. Again, that's why it's called a stack. So for the stylistic ones, let's say I have one for make the writing as formal as possible, the intended stack would be send in the remove the filler words, add the punctuation, add the paragraphs, then make the language as formal as possible. So they're additive to the foundational. And ideally in the, in the front end, the prompt construction or concatenation logic that I've developed, there also you can use multiple ones and that's where the order of construction again becomes kind of important because in addition to just defining them, you want to present them in a logical manner. So to give another example of a of a additive stack that has elements to it, we would have I want it to be formal and persuasive. Two things. Formal is a is writing style. Persuasive is a another writing style that's going to affect the selection of language. So that stack might be called in the front end label formal and persuasive. And the actual stack or prompt rewriting stack that would get sent to the audio multimodal model in this case would be one, here's the audio data. Two, the user's name is Daniel. This is being injected through the personalization. Three, here are the general instructions for how you should transcribe this audio into text, going through all the foundational elements we've discussed here. Four, make the text as formal as possible. Five, make the text as persuasive as possible.

And what we want in the prompt concatenation logic is probably to, we don't want firstly to introduce the more elaborate this becomes, the greater the chance that we're going to have internal logical conflicts. Like we can't have if the main cleaner prompt said it should be as easy to read as possible and then we had it should be as formal as possible, they're kind of oppositional. So, you know, without getting too lost in the in the complication of this idea, the actual prompt construction logic might itself need to be relied upon AI to say, you know, you've got these three ingredients as it were and we want to create a constructed prompt. And the answer might be actually that these constructed metaprompts, instead of being injected dynamically, are predefined so that if there are any potential internal disagreements that they're resolved and that whatever tool that I'm using for this transcription process has a bunch of presets ready that are validated that they everything's in harmony, it's doing all the jobs and you just send something and you get back something exactly meeting our expectations. That's the objective here. So I'll have to leave it at that as I'm getting fatigued from carrying around this little guy. But that's the idea here and this will be used this lengthy audio file, which is actually quite reflective. It's about 25 minutes. We're going to hit 30 probably. It'll be good to to demonstrate two things. Number one, that audio multimodal is very capable. I've used it for 1 hour. The challenge is actually, there is a theoretical audio length limit. Sorry, not theoretical, there's documented, but it's like multiple hours. It's very unlikely that someone's going to hit it in this use. The more pertinent one is a audio file size limit. I think Gemini is 20 megabytes. So what I do in my implementation is to do preprocessing, VAD to cut out silence, and then getting it down to audio friendly format and that generally means going from stereo to mono. What else do I do? Compressing it to Opus. And usually that's enough. You can actually, that's generally enough that if you record something in Wave, for audio it's going to be down sampled anyway, so there's literally no point. You do these preprocessing steps and then you hit the API well under the limit, well under the audio size limit and you get back audio processing. That's where the final thing I'll say, I tried local models for this. There are a few that can run, and it just wasn't possible because I think this approach just is very computationally heavy. In the sense that if you're sending something in one go, chunking doesn't need even really make sense. So you're asking it to hold a huge amount of context there. And I find with my own attempts, I think I got VaxRunning, but it just couldn't I couldn't contain it. Like the the GPU just ran out of memory. And even with kind of attempts to use the chunking, that kind of had its own problems. So I just because the costs are so little as well, it just it's not an approach I'm that I'm really going to look into at all. And in general, I'm not. My preference for voice stuff, I'm actually much prefer to use cloud inference.

I just think and I understand there's there's different reasons people don't want to do that like privacy. I'm not worried about that particularly, so it's for me just the costing doesn't really justify the the complexity and more than the complexity the using those less performant models. So that's all the details about this process. We will send it forward the transcription now.